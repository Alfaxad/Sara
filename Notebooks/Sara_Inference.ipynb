{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZxDZpsNIC7w"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet transformers accelerate torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "MODEL_ID = \"Nadhari/Sara-1.5-4B-it\"\n",
        "\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "print(f\"Model loaded: {MODEL_ID}\")\n",
        "print(f\"Device: {model.device}\")"
      ],
      "metadata": {
        "id": "_jaOmeJFIHVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"You are an expert in using FHIR functions to assist medical professionals. You are given a question and a set of possible functions. Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n",
        "\n",
        "1. If you decide to invoke a GET function, you MUST put it in the format of\n",
        "GET url?param_name1=param_value1&param_name2=param_value2...\n",
        "\n",
        "2. If you decide to invoke a POST function, you MUST put it in the format of\n",
        "POST url\n",
        "[your payload data in JSON format]\n",
        "\n",
        "3. If you have got answers for all the questions and finished all the requested tasks, you MUST call to finish the conversation in the format of\n",
        "FINISH([answer1, answer2, ...])\n",
        "\n",
        "Your response must be in the format of one of the three cases, and you can call only one function each time. You SHOULD NOT include any other text in the response.\n",
        "\n",
        "Available FHIR endpoints (use http://localhost:8080/fhir/ as api_base):\n",
        "- GET {api_base}/Patient?given=&family=&birthdate= (Search patients)\n",
        "- GET {api_base}/Observation?code=&patient=&date= (Query labs/vitals)\n",
        "- POST {api_base}/Observation (Record vitals)\n",
        "- POST {api_base}/MedicationRequest (Order medications)\n",
        "- POST {api_base}/ServiceRequest (Order referrals/labs)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "XnAT2EF-IK7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(messages, max_new_tokens=512):\n",
        "    \"\"\"Generate a single response from the model.\"\"\"\n",
        "    input_text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    input_len = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)\n",
        "    return response.strip()"
      ],
      "metadata": {
        "id": "BFUzApYRINun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task: Find patient MRN by name and DOB\n",
        "question = \"What's the MRN of the patient with name Maria Garcia and DOB of 1990-07-22?\"\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": SYSTEM_PROMPT + \"\\n\\nQuestion: \" + question}]\n",
        "response = generate_response(messages)\n",
        "\n",
        "print(\"Task: Patient MRN Lookup\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Agent Response:\\n{response}\")"
      ],
      "metadata": {
        "id": "X9hhF8qhITdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task: Record blood pressure vital signs\n",
        "question = \"\"\"Record the blood pressure of patient S1234567.\n",
        "The systolic is 120 and the diastolic is 80.\n",
        "Use the current datetime 2024-01-15T10:30:00+00:00.\"\"\"\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": SYSTEM_PROMPT + \"\\n\\nQuestion: \" + question}]\n",
        "response = generate_response(messages)\n",
        "\n",
        "print(\"Task: Record Blood Pressure\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Agent Response:\\n{response}\")"
      ],
      "metadata": {
        "id": "mNO2ywvhIULi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate a full multi-turn agent workflow\n",
        "print(\"Task: Full Agent Workflow - Patient Lookup\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Turn 1: Initial question\n",
        "question = \"What's the MRN of the patient with name John Smith and DOB of 1985-03-15?\"\n",
        "conversation = [{\"role\": \"user\", \"content\": SYSTEM_PROMPT + \"\\n\\nQuestion: \" + question}]\n",
        "\n",
        "print(f\"\\n[Turn 1] User Question:\\n{question}\")\n",
        "agent_response = generate_response(conversation)\n",
        "print(f\"\\n[Turn 1] Agent Response:\\n{agent_response}\")\n",
        "\n",
        "# Simulate FHIR server response\n",
        "conversation.append({\"role\": \"model\", \"content\": agent_response})\n",
        "fhir_response = \"\"\"Here is the response from the GET request:\n",
        "{\n",
        "  \"resourceType\": \"Bundle\",\n",
        "  \"type\": \"searchset\",\n",
        "  \"total\": 1,\n",
        "  \"entry\": [{\n",
        "    \"resource\": {\n",
        "      \"resourceType\": \"Patient\",\n",
        "      \"id\": \"S6534835\",\n",
        "      \"identifier\": [{\"type\": {\"coding\": [{\"code\": \"MR\"}]}, \"value\": \"S6534835\"}],\n",
        "      \"name\": [{\"family\": \"Smith\", \"given\": [\"John\"]}],\n",
        "      \"birthDate\": \"1985-03-15\"\n",
        "    }\n",
        "  }]\n",
        "}. Please call FINISH if you have got answers for all the questions.\"\"\"\n",
        "\n",
        "conversation.append({\"role\": \"user\", \"content\": fhir_response})\n",
        "print(f\"\\n[Turn 2] FHIR Response: (Bundle with 1 patient)\")\n",
        "\n",
        "# Turn 2: Agent extracts answer\n",
        "agent_response = generate_response(conversation)\n",
        "print(f\"\\n[Turn 2] Agent Response:\\n{agent_response}\")"
      ],
      "metadata": {
        "id": "jhHftKA0IWu0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}