Rank,Model,Accuracy (%),Correct,Total,Completed,Invalid Actions,Limit Reached,Errors,Time (min)
1,Claude Opus 4.5,95.0,285,300,300,0,0,0,27.8
2,Gemini 3 Flash,90.0,269,299,293,0,0,6,26.1
3,x-ai/grok-4.1-fast,87.7,263,300,300,0,0,0,73.0
4,Claude Sonnet 4.5,86.0,258,300,300,0,0,0,25.8
5,GPT-4.1,77.3,232,300,300,0,0,0,15.6
6,GPT-4o,77.0,231,300,300,0,0,0,16.1
7,GPT-5.1,76.0,228,300,300,0,0,0,18.2
8,qwen/qwen3-14b,74.0,222,300,275,23,2,0,180.0
9,Gemini 2.5 Flash,72.8,217,298,298,0,0,0,14.8
10,Sara 1.5 4B,66.7,200,300,300,0,0,0,19.8
11,mistralai/mistral-small-3.2-24b-instruct,51.7,155,300,300,0,0,0,26.4
12,Llama 4 Scout,41.3,124,300,262,34,4,0,17.1
13,qwen/qwen3-8b,40.0,120,300,199,93,8,0,500.0
14,Llama 4 Maverick,37.3,112,300,183,116,0,1,21.0
15,meta-llama/llama-3.1-8b-instruct,14.0,42,300,199,14,87,0,75.2
