# Sara 1.5 4B: MedAgentBench Results

## Benchmark Overview

**MedAgentBench** evaluates LLM agents on 300 clinical EHR tasks against a FHIR R4 server with 100 patient profiles (700K+ records). Each task requires the model to interact with the EHR through HTTP requests (GET/POST) and return structured answers. Evaluation uses pass@1 (single attempt) with a maximum of 8 interaction rounds per task.

### Task Types (10 categories, 30 tasks each)

| Task | Type | Description |
|------|:----:|-------------|
| Patient Search | Query | Find patient by name/DOB, return MRN |
| Lab Result Retrieval | Query | Calculate patient age from birthdate |
| Medication Verification | Action | Record blood pressure measurement |
| Allergy Information | Query | Retrieve recent magnesium level |
| Condition Lookup | Action | Check Mg level, order replacement if low |
| Vital Signs Retrieval | Query | Calculate average CBG over 24 hours |
| Clinical Note Creation | Query | Retrieve most recent CBG value |
| Immunization Records | Action | Create orthopedic surgery referral |
| Procedure History | Action | Check K+ level, order replacement + recheck |
| Care Plan Management | Action | Check HbA1C, order new test if stale |

---

## Sara 1.5 4B Performance

**Overall Accuracy: 66.7%** (200/300 tasks correct)

### Efficiency

Sara achieves **16.67 accuracy points per billion parameters** — the highest efficiency of all 15 benchmarked models and **3x more efficient** than the next-best model (Qwen3-14B at 5.29).

### Small Model Leaderboard (≤30B Parameters)

| Rank | Model | Parameters | Accuracy |
|:----:|-------|:----------:|:--------:|
| 1 | Qwen3-14B | 14B | 74.0% |
| **2** | **Sara 1.5 4B** | **4B** | **66.7%** |
| 3 | Mistral Small 3.2 | 24B | 51.7% |
| 4 | Qwen3-8B | 8B | 40.0% |
| 5 | Llama 3.1-8B Instruct | 8B | 14.0% |

Sara is the **smallest model** yet achieves the **second-highest accuracy** among models ≤30B parameters.

---

## State-of-the-Art Tasks

Sara ranks **#1 among all 15 models** (including frontier models) on these tasks:

| Task | Sara | Runner-up | Margin |
|------|:----:|:---------:|:------:|
| **Procedure History** | **96.7%** | Claude Opus 4.5 (93.3%) | +3.4% |
| Patient Search | 100% | Tied with 6 models | — |
| Allergy Information | 100% | Tied with 3 models | — |
| Immunization Records | 100% | Tied with 6 models | — |

**Procedure History** is the benchmark's most complex multi-step task, requiring retrieval, conditional reasoning, dose calculation, and generation of two POST requests. Sara's 96.7% accuracy **surpasses all frontier models** including Claude Opus 4.5.

---

## Format Compliance

Sara achieved **zero invalid actions** across all 300 tasks — perfect tool-calling format adherence matching frontier models like Claude Opus 4.5 and Grok 4.1 Fast.

---

## Outperforming Larger Models

Sara (4B parameters) outperforms five larger models: **Llama 4 Maverick** (400B, 100x larger) at 37.3%, **Llama 4 Scout** (109B, 27x larger) at 41.3%, **Mistral Small 3.2** (24B, 6x larger) at 51.7%, **Qwen3-8B** (8B, 2x larger) at 40.0%, and **Llama 3.1-8B Instruct** (8B, 2x larger) at 14.0%. Combined, Sara outperforms models totaling **549 billion parameters** — 137x its own size.

---

## Training

Sara is fine-tuned from **MedGemma-1.5-4B-it** on **284 correct interaction traces** generated by Claude Opus 4.5 on MedAgentBench tasks. The base MedGemma model scores 0% on the benchmark (no knowledge of the tool-calling format), making Sara's 66.7% accuracy a direct measure of what targeted fine-tuning on a small dataset can achieve.

---

*Full benchmark methodology and 15-model leaderboard available in [BENCHMARKING_REPORT.md](../BENCHMARKING_REPORT.md)*
